{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import product\n",
    "from typing import List\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_lg-3.4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class POS(Enum):\n",
    "\tVERB = wordnet.VERB\n",
    "\tNOUN = wordnet.NOUN\n",
    "\tADJ = wordnet.ADJ\n",
    "\tADV = wordnet.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR = False\n",
    "#Word class to capture everything related to a word\n",
    "class Word:\n",
    "\tdef __init__(self,spacy_token:str):\n",
    "\t\tself.raw_word = spacy_token.text\n",
    "\t\tself.smallcase_word = spacy_token.text.lower()\n",
    "\t\t#Tag lemma\n",
    "\t\tself.lemma = spacy_token.lemma_\n",
    "\t\t#Tag parts of speech\n",
    "\t\tself.pos = spacy_token.pos_\n",
    "\t\t#Antonyms of the word\n",
    "\t\tself.antonyms = []\n",
    "\t\t#Get wordnet synnets by pos\n",
    "\t\tself.synnets = []\n",
    "\t\tself.lemma_list = []\n",
    "\n",
    "\t\tif self.pos == 'VERB' or self.pos == 'NOUN' or self.pos == 'ADJ' or self.pos == 'ADV':\n",
    "\t\t\tself.get_synnets()\n",
    "\t\t\n",
    "\t\tself.find_antonyms()\n",
    "\t\t\n",
    "\t\t# self.print_synnets()\n",
    "\t\n",
    "\t#Get the synnets for the word (Wordnet) - Use spaCy POS tag\n",
    "\tdef get_synnets(self):\n",
    "\t\tpos = self.pos\n",
    "\t\t# print(f\"Finding synnets for word: {self.raw_word} POS identified by spaCy: {self.pos}\") if PR else 0\n",
    "\t\tself.synnets = wordnet.synsets(self.raw_word)\n",
    "\t\t# print(f\"Total synnets found = {len(self.synnets)}\") if PR else 0\n",
    "\n",
    "\tdef print_synnets(self):\n",
    "\t\tfor synnet in self.synnets:\n",
    "\t\t\tprint(f\"{synnet} - {synnet.definition()}\") if PR else 0\n",
    "\t\t\t\n",
    "\tdef __str__(self):\n",
    "\t\treturn f\"{self.raw_word} | {self.lemma} | {self.pos} | {len(self.synnets)}\"\n",
    "\n",
    "\tdef find_lemmas(self):\n",
    "\t\tfor synnet in self.synnets:\n",
    "\t\t\tself.lemma_list = [lemma for lemma in synnet.lemmas()]\n",
    "\t\t\tprint(f\"Total lemmas found for word : {self.raw_word} = {len(self.lemma_list)}\")\n",
    "\t\n",
    "\tdef find_antonyms(self):\n",
    "\t\tfor syn in self.synnets:\n",
    "\t\t\tfor lem in syn.lemmas():\n",
    "\t\t\t\tif lem.antonyms():self.antonyms.append(lem.antonyms()[0].name().lower())\n",
    "\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "\n",
    "\t\"\"\"\n",
    "\tSentence class wrapper to store the complete sentence information\n",
    "\n",
    "\tVars: \n",
    "\traw_sentense : Complete sentense as a string\n",
    "\tdoc          : spaCy parsed pipeline\n",
    "\tword_list    : Word class objects list\n",
    "\tstop_words   : List of stop words \n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, string:str):\n",
    "\t\tself.raw_sentense = string\n",
    "\n",
    "\t\t#Apply spaCy NLP to tokenize and extract information\n",
    "\t\tself.doc = nlp(string)\n",
    "\t\t\n",
    "\t\t#Display spaCy o/p\n",
    "\t\tprint(\"\\nText  |  Lemma  |  POS  |  Tag  |  Dep  |  Shape  |  Alpha  |  Stop  |\") if PR else 0\n",
    "\t\tself.disp()\n",
    "\n",
    "\t\t#Extract subject , object, verb\n",
    "\t\tself.extract_concepts()\n",
    "\n",
    "\t\t#Create new Word wrapper for each tokens in the sentense and dont add stop words\n",
    "\t\tself.word_list: List[Word] = []\n",
    "\n",
    "\t\tself.stop_words = []\n",
    "\t\t\n",
    "\t\t# print(\"*************** Removing stop words and punctuation and creating Word objects ***************************\")\n",
    "\t\t#TODO : add custom stop words remover -> Improve performance\n",
    "\t\tfor token in self.doc:\n",
    "\t\t\t#Remove stop words\n",
    "\t\t\tif token.is_stop:\n",
    "\t\t\t\tprint(f'Stop word removed: {token.text}') if PR else 0\n",
    "\t\t\t\tself.stop_words.append(token)\n",
    "\t\t\telif token.is_punct:\n",
    "\t\t\t\tprint(f\"Punctuation removed : {token.text}\") if PR else 0\n",
    "\t\t\telse:self.word_list.append(Word(token))\n",
    "\t\n",
    "\t#Display spaCy pipeline output\n",
    "\tdef disp(self):\n",
    "\t\tfor token in self.doc:\n",
    "\t\t\t#Text |\tLemma |\tPOS | Tag |\tDep | Shape | Alpha | Stop\n",
    "\t\t\tprint(token.text,'|', token.lemma_,'|', token.pos_,'|', token.tag_,'|', token.dep_,'|',token.shape_,'|', token.is_alpha,'|', token.is_stop) if PR else 0\n",
    "\t\n",
    "\tdef extract_concepts(self):\n",
    "\t\t#Extract subject, object, root/verb.. etc\n",
    "\t\tfor token in self.doc:\n",
    "\t\t\tif token.dep_ == 'nsubj':\n",
    "\t\t\t\tself.subject = token.text\n",
    "\t\t\telif token.dep_ == 'dobj':\n",
    "\t\t\t\tself.obj = token.text\n",
    "\t\t\telif token.dep_ == 'ROOT':\n",
    "\t\t\t\tself.verb = token.text\n",
    "\n",
    "\n",
    "\tdef get_word_set(self):\n",
    "\t\tword_list = [ word.smallcase_word for word in self.word_list ]\n",
    "\t\treturn set(word_list)\n",
    "\t\n",
    "\tdef get_word_list(self):\n",
    "\t\treturn [ word for word in self.word_list ]\n",
    "\t\n",
    "\tdef get_str_word_list(self):\n",
    "\t\treturn [ word.smallcase_word for word in self.word_list ]\n",
    "\n",
    "\tdef get_all_synnets(self):\n",
    "\t\tsyn_lst = [ word.synnets for word in self.word_list]\n",
    "\t\treturn syn_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper:\n",
    "    \n",
    "    def __init__(self, phrase1, phrase2):\n",
    "        #Create sentense objects for both phrases\n",
    "        self.sentence1 = Sentence(phrase1)\n",
    "        self.sentence2 = Sentence(phrase2)\n",
    "        self.overall_score = 0\n",
    "        self.has_antonyms = False\n",
    "        self.word_order_score = 0\n",
    "        self.vector_score = 0\n",
    "        self.reason = ''\n",
    "    \n",
    "    def calculate_similarity_score(self):\n",
    "        #Pipeline of checks and calculations\n",
    "\n",
    "\t\t#1. Check for antonym -> If antonyms are found - make similarity score zero\n",
    "        self.check_antonym()\n",
    "        if self.has_antonyms:\n",
    "            print(\"Reason : Antonym , Similarity score = 0\")\n",
    "            self.reason = 'Antonym'\n",
    "            return\n",
    "\n",
    "        lmbda = 0.6 #Can be set between 0 & 1\n",
    "\t\t#2. Get vector + path similarity score\n",
    "        s_c = self.vector_similarity_score()\n",
    "        self.vector_score = s_c\n",
    "        \n",
    "\t\t#3. Get word order similarity score\n",
    "        s_w = self.word_order_similarity()\n",
    "        self.word_order_score = s_w\n",
    "\n",
    "        score = math.sqrt( (math.pow(lmbda,2) * s_w ) + ( (1 - math.pow(lmbda,2)) * s_c ))\n",
    "        self.overall_score = score\n",
    "        print(f\"Final score : {score}\")\n",
    "\t\t\t\n",
    "                \n",
    "    def check_antonym(self):\n",
    "        word_list1 = self.sentence1.get_word_list()\n",
    "        word_list2 = self.sentence2.get_word_list()\n",
    "\t\n",
    "\t\t#For every word in s1 check if the other s2 has any antonyms\n",
    "\t\t#TODO: Only check for VERB-VERB, NOUN-NOUN words for antonym: Improve efficiency\n",
    "        for word1 in word_list1:\n",
    "            for word2 in word_list2:\n",
    "                if word1.smallcase_word in word2.antonyms:\n",
    "                    print(f'Found antonym for word {word1.raw_word}')\n",
    "                    self.has_antonyms=True\n",
    "    \n",
    "\t#Combination of vector and wordnet path similarity\n",
    "\t#TODO: Compare/Calculate scores for only matching words - (sub-sub / obj-obj / verb-verb...)\n",
    "    #TODO: If wordnet score is zero for any two words then dont take vector similarity (hard set score to 1.0)\n",
    "    def vector_similarity_score(self):\n",
    "        sim_score1, simscore2 = 0.0, 0.0\n",
    "        score1, score2, avr_score1, avr_score2 = 0.0, 0.0, 0.0, 0.0\n",
    "        score, count1, count2 = 0.0, 0, 0\n",
    "\n",
    "        #Get all words\n",
    "        lst1 = self.sentence1.get_word_list()\n",
    "        lst2 = self.sentence2.get_word_list()\n",
    "\n",
    "        #For each word in sentense 1 find maximum of sum of vector and semantic similarity with all the words in sentense 2\n",
    "        for word1 in lst1:\n",
    "            sim_score1 = max([ ( (  self.spacy_vector_similarity(word,word1) + self.get_path_similarity(word, word1)) / 2 ) for word in lst2])\n",
    "            if sim_score1 is not None:\n",
    "                score1 += sim_score1\n",
    "                count1 += 1\n",
    "        avr_score1 = score1 / count1\n",
    "\n",
    "        #For each word in sentense 2 find maximum of sum of vector and semantic similarity with all the words in sentense 1\n",
    "        for word1 in lst2:\n",
    "            sim_score2 = max([ ( (  self.spacy_vector_similarity(word,word1) + self.get_path_similarity(word, word1)) / 2 ) for word in lst1])\n",
    "            if sim_score2 is not None:\n",
    "                score2 += sim_score2\n",
    "                count2 += 1\n",
    "        avr_score2 = score2 / count2\n",
    "\n",
    "        #Get average of the max scores for the sentense\n",
    "        score = (avr_score1 + avr_score2) / 2\n",
    "        print(f\"Combined word vector + Wordnet similarity score = {score}\") if PR else 0\n",
    "        return score\n",
    "\n",
    "    def spacy_vector_similarity(self, word1:Word, word2:Word):\n",
    "        doc1 = nlp(word1.smallcase_word)\n",
    "        doc2 = nlp(word2.smallcase_word)\n",
    "        score = doc1.similarity(doc2)\n",
    "        print(f\"SpaCy vector similarity :{word1.smallcase_word} - {word2.smallcase_word} = {score}\") if PR else 0\n",
    "        return score\n",
    "    \n",
    "    def get_path_similarity(self, word1:Word, word2:Word):\n",
    "        max_score = 0\n",
    "        net1 = ''\n",
    "        net2 = ''\n",
    "        for synnet1 in word1.synnets:\n",
    "            for synnet2 in word2.synnets:\n",
    "                path_sim = wordnet.path_similarity(synnet1, synnet2)\n",
    "                # print(f\"{synnet1} - {synnet2} : {path_sim}\") if PR else 0\n",
    "                if path_sim > max_score: \n",
    "                    max_score = path_sim\n",
    "                    net1 = synnet1\n",
    "                    net2 = synnet2\n",
    "        print(f\"Max path similarity score {net1} , {net2} : {max_score}\") if PR else 0\n",
    "        return max_score\n",
    "\n",
    "\t#TODO: send application form, send application -> Should give maximum word order similarity. Sort in some way\n",
    "    #TODO: Include stop words also?!!!\n",
    "    def word_order_similarity(self):\n",
    "        count = 0\n",
    "        #Get string word list\n",
    "        str_word_list1 = self.sentence1.get_str_word_list()\n",
    "        str_word_list2 = self.sentence2.get_str_word_list()\n",
    "\n",
    "\t\t#Combine the word lists, sort and convert to set\n",
    "        str_word_list = str_word_list1 + str_word_list2\n",
    "        # print(str_word_list)\n",
    "        str_word_list.sort()\n",
    "\n",
    "        # print(str_word_list)\n",
    "        word_set = set(str_word_list)\n",
    "        # word_set = \n",
    "        \n",
    "        # word_set = self.sentence1.get_ordered_word_set().union(self.sentence2.get_ordered_word_set())\n",
    "        vector1 = [0] * len(word_set)\n",
    "        vector2 = [0] * len(word_set)\n",
    "\n",
    "        print(f\"Union word list:{word_set}\")\n",
    "\n",
    "        for word in word_set:\n",
    "            count+= 1\n",
    "            if word in str_word_list1:\n",
    "\t\t\t#Get index\n",
    "                ix = str_word_list1.index(word)\n",
    "                vector1[ix] = count\n",
    "            if word in str_word_list2:\n",
    "                ix = str_word_list2.index(word)\n",
    "                vector2[ix] = count\n",
    "        print(f\"Word order vectors formed :{vector1} {vector2}\") if PR else 0\n",
    "\n",
    "        vector_sum = np.add(vector1,vector2)\n",
    "        vector_sub = np.subtract(vector1,vector2)\n",
    "\n",
    "\t    # word order score Sw = 1 - ( |V1-V2| / |V1+V2| )\n",
    "        wo_score = 1 - ( np.linalg.norm(vector_sub) / np.linalg.norm(vector_sum))\n",
    "        print(f\"Word order score = {wo_score}\") if PR else 0\n",
    "        return wo_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wup_similarity(word1: Word, word2: Word):\n",
    "\tmax_score = 0\n",
    "\t# print(\"*******************FINDING WUP SIMILARITY*******************\") if PR else 0\n",
    "\tfor synnet1 in word1.synnets:\n",
    "\t\tfor synnet2 in word2.synnets:\n",
    "\t\t\twup_sim = wordnet.wup_similarity(synnet1, synnet2)\n",
    "\t\t\tprint(f\"{synnet1} - {synnet2} : {wup_sim}\") if PR else 0\n",
    "\t\t\tif wup_sim > max_score: \n",
    "\t\t\t\tmax_score = wup_sim\n",
    "\tprint(f\"Max score : {max_score}\") if PR else 0\n",
    "\treturn max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR = True\n",
    "s1 = \"Verify invoice\"\n",
    "s2 = \"Check bill\"\n",
    "wr = Wrapper(s1,s2)\n",
    "wr.calculate_similarity_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "PR = False\n",
    "import json\n",
    "f = open('mapping_87.json')\n",
    "data = json.load(f)\n",
    "\n",
    "pair = data['Mapping']\n",
    "\n",
    "#TODO: Needs more cleaning up\n",
    "lst = []\n",
    "for att,value in pair.items():\n",
    "    for an, vl in value.items():\n",
    "    \tif an == 'label':\n",
    "            if vl != \"\" and vl != \" \" and vl != None:\n",
    "                #Check for regex (No special symbols and numbers)\n",
    "                if re.match(r\"^[a-zA-Z\\s]+$\", vl):\n",
    "                    lst.append(vl)\n",
    "                else: print(f\"Invalid sentence input:{vl}\")\n",
    "        \n",
    "print(f'Total sentences found:{len(lst)}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "\n",
    "for i in range(0,200):\n",
    "    for j in range(i,200):\n",
    "        # print(f'Phrase 1: {lst[i]}')\n",
    "        # print(f'Phrase 2: {lst[j]}')\n",
    "        st1 = lst[i]\n",
    "        st2 = lst[j]\n",
    "        wr = Wrapper(st1,st2)\n",
    "        try:\n",
    "            wr.calculate_similarity_score()\n",
    "            row = [st1, st2, wr.reason, wr.vector_score, wr.word_order_score, wr.overall_score]\n",
    "            df.append(row)\n",
    "        except:\n",
    "             print(\"Exception :{st1}, {st2}\")\n",
    "        # Phrase 1, Phrase 2, reson, vector score, word order score, overall score \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe and write to excel\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "dataframe = pd.DataFrame(df, columns =['phrase_1', 'phrase_2', 'reason', 'vector_sim_score', 'word_order_score', 'overall_score'])\n",
    "# dataframe.show()\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('out.xlsx', engine='xlsxwriter')\n",
    "dataframe.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(r'out.xlsx')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Verify invoice', 'Check invoice']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "processed_phrases = []\n",
    "phrases = ['Verify invoice', 'Check invoice']\n",
    "for phrase in phrases:\n",
    "\tdoc = nlp(phrase)\n",
    "\twords = []\n",
    "\tfor token in doc:\n",
    "\t\tif not token.is_stop and token.is_alpha:\n",
    "\t\t\tword = lemmatizer.lemmatize(token.text)\n",
    "\t\t\tsynset = wordnet.synsets(word)\n",
    "\t\t\tsyn_words = set([l.name() for s in synset for l in s.lemmas() if l.name() != word])\n",
    "\t\t\thypernyms = set([h for s in synset for h in s.hypernyms()])\n",
    "\t\t\thyponyms = set([h for s in synset for h in s.hyponyms()])\n",
    "\t\t\twords.append((word, syn_words, hypernyms, hyponyms))\n",
    "\tprocessed_phrases.append(words)\n",
    "processed_phrases\n",
    "corpus = [' '.join([w[0] for w in words]) for words in processed_phrases]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'phrase1': ['Verify invoice', 'Check invoice'], 'phrase2': ['Verify invoice', 'Check invoice'], 'score':[0.7,0.8]}\n",
    "df = pd.DataFrame(data=d)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "model = LinearRegression()\n",
    "model.fit(X[df['phrase1'],:], df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "processed_sentences = [preprocess_text(sentence) for sentence in ['Verify invoice', 'Check invoice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4f85d6e04298634172ac5d8264e7e9b556b95639fe52ebb9425c4d4cba0c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
